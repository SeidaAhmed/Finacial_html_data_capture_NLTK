{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeidaAhmed/Finacial_html_data_capture_NLTK/blob/main/CRWD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZLfvIE8EOF9p"
      },
      "outputs": [],
      "source": [
        "metrics_8k=[\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552725000005/crwd-20250304xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552724000024/crwd-20241126xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552724000017/crwd-20240828xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552724000011/crwd-20240604xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552724000004/crwd-20240305xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552723000024/crwd-20231128xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552723000018/crwd-20230830xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552723000012/crwd-20230531xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552723000006/crwd-20230307xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552722000023/crwd-20221031xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552722000016/crwd-20220731xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552722000010/crwd-20220430xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552722000004/crwd-20220309xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552721000026/crwd-20211201xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552721000017/crwd-20210831xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552721000011/crwd-20210603xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552721000005/crwd-20210316xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552720000023/crwd-20201202xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552720000017/crwd-20200902xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552720000010/crwd-20200602xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552720000004/crwd-20200319xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000153552719000007/crwd-20191205xex991.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000155837019008509/ex-99d1.htm\",\n",
        "\"https://www.sec.gov/Archives/edgar/data/1535527/000155837019006148/ex-99d1.htm\"\n",
        "]\n",
        "\n",
        "metrics_8k=['https://www.sec.gov/Archives/edgar/data/1535527/000153552725000005/crwd-20250304xex991.htm']\n",
        "\n",
        "\n",
        "#Annual Recurring Revenue (ARR)\n",
        "#gross retention\n",
        "# Next-Gen SIEM, Cloud Security, and Identity Protection businessesARR\n",
        "#total subscription customers\n",
        "#rowdStrike’s subscription customers that have adopted four or more modules, five or more modules and six or more modules increased to 64%, 50%, and 27%, respectively, as of April 30, 2021.\n",
        "#                                    Must capture--> four modules 64 %,   five: 50% six 27%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coB_5a08PETP"
      },
      "source": [
        "Important imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBr5AhLKPJsL",
        "outputId": "ec9f9145-b537-46aa-fcb6-8169e135d7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data] Error loading path: Package 'path' not found in index\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "try:\n",
        "    import urllib.request as urllib3\n",
        "    #requests.packages.urllib3.contrib.pyopenssl.extract_from_urllib3()\n",
        "except ImportError:\n",
        "    import urllib3\n",
        "import zlib\n",
        "from nltk import Tree\n",
        "import re\n",
        "nltk.download('maxent_treebank_pos_tagger')\n",
        "nltk.download('path')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "years = []\n",
        "for i in range(2000,2100):\n",
        "  years.append(i)\n",
        "unit_conv={\"billion\":1000000000, \"million\":1000000,\"millions\":1000000,\"thousand\":1000,\"%\":0.01,\"percentage\":0.01,\"percent\":0.01,\"\":1,\"basis points\":0.0001,\"bps\":0.0001}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uL7mmtLP3I0"
      },
      "source": [
        "Functions that should generally stay the same:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uXtUmNvJP6uI"
      },
      "outputs": [],
      "source": [
        "def extract_soup(url):\n",
        "\n",
        "    hdr = {'User-Agent': 'UbineerCorp info@ubineer.com','Accept-Encoding': 'gzip, deflate','host': 'www.sec.gov'}\n",
        "    s = requests.Session()\n",
        "    s.headers.update(hdr)\n",
        "    data = s.get(url)\n",
        "    s.close()\n",
        "    soup = BeautifulSoup(data.content, features=\"html.parser\")\n",
        "    return(soup)\n",
        "\n",
        "def openBS(html_file):\n",
        "\n",
        "  with open(html_file, encoding = \"ISO-8859-1\") as fp:\n",
        "\n",
        "    soup = BeautifulSoup(fp,'html.parser')\n",
        "\n",
        "  return soup\n",
        "\n",
        "def find_between( s, first, last ):\n",
        "    try:\n",
        "        start = s.index( first ) + len( first )\n",
        "        end = s.index( last, start )\n",
        "        return s[start:end]\n",
        "    except ValueError:\n",
        "        return \"\"\n",
        "\n",
        "def find_between_include(s, first, last, length):\n",
        "    try:\n",
        "        start = s.index( first ) - length\n",
        "        end = s.index( last, start )\n",
        "        return s[start:end]\n",
        "    except ValueError:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def unitConversion(insert_lst):\n",
        "  value = None\n",
        "  insert_lst[0] = insert_lst[0].replace('flat','0%')\n",
        "  unit_conv={\"billion\":1000000000,\"million\":1000000,\"thousand\":1000,\"%\":0.01,\"percentage\":0.01,\"percent\":0.01,\"\":1,\"basis points\":0.0001,\"bps\":0.0001}\n",
        "  if len(insert_lst) != 2:\n",
        "\n",
        "    for unit in list(unit_conv.keys()):\n",
        "      if unit == '':\n",
        "        pass\n",
        "      elif unit in insert_lst[0]:\n",
        "        insert_lst = [str(insert_lst[0]).replace(unit,''), unit]\n",
        "\n",
        "\n",
        "\n",
        "  insert_lst[-1] =insert_lst[-1].lower()\n",
        "\n",
        "\n",
        "  insert_lst[0] = str(insert_lst[0]).replace(',','').replace('$','').replace('(','-').replace(')','').replace('%','').strip()\n",
        "  #Andi adds brackets\n",
        "\n",
        "\n",
        "  insert_lst[0] = float(insert_lst[0])\n",
        "\n",
        "  for ele in unit_conv.keys():\n",
        "    #print(ele)\n",
        "    if ele == insert_lst[-1]:\n",
        "      value = insert_lst[0] * unit_conv[ele]\n",
        "      #attempt to fix floating point error\n",
        "      value = round(value,4)\n",
        "    else:\n",
        "      pass\n",
        "  if value == None:\n",
        "    print(\"Value is None: \", insert_lst)\n",
        "  return value\n",
        "\n",
        "def change_pos_tag(tag_sentence,model):\n",
        "    new_tag_lst=[]\n",
        "    for ele in tag_sentence:\n",
        "        if ele[0] in model.keys():\n",
        "            new_tag_lst.append((ele[0],model[ele[0]]))\n",
        "        else:\n",
        "            new_tag_lst.append(ele)\n",
        "    return(new_tag_lst)\n",
        "\n",
        "def listofdict (tree):\n",
        "  # source: https://stackoverflow.com/questions/63945863/converting-nltk-chunks-to-a-list-of-dictionaries\n",
        "  # need helper function to simply put chunks into dictionaries\n",
        "    dlist = []\n",
        "    d = {}\n",
        "    for item in tree:\n",
        "\n",
        "        if isinstance(item, Tree):\n",
        "\n",
        "            d[item.label()] = ' '.join([l[0] for l in item.leaves()])\n",
        "            dlist.append(d) if len(d)>0 else None\n",
        "            d = dict()\n",
        "\n",
        "    dlist.append(d) if len(d)>0 else None\n",
        "    return dlist\n",
        "\n",
        "def find_segment(listofdict):\n",
        "    # helper function\n",
        "    tl = \"Timeline\"\n",
        "    tl_count = 0\n",
        "    segment = 0\n",
        "    for i in range(len(listofdict)):\n",
        "        if tl in listofdict[i].keys():\n",
        "            tl_count +=1\n",
        "            if tl_count == 2:\n",
        "                segment = i\n",
        "                return segment\n",
        "    return len(listofdict)\n",
        "\n",
        "def combine_dict(listofdict):\n",
        "  # merge messy dictionaries from stack overflow function\n",
        "    master_outlook = []\n",
        "\n",
        "    while(len(listofdict) > 0 ):\n",
        "        segment = find_segment(listofdict)\n",
        "\n",
        "        time_dict = {}\n",
        "        for i in range(segment):\n",
        "            time_dict.update(listofdict[i])\n",
        "\n",
        "        listofdict = listofdict[segment:]\n",
        "        master_outlook.append(time_dict)\n",
        "    return master_outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYLDuNS4R5j7"
      },
      "source": [
        "Capture guidance function: This function captures all the text in the guidance section of the html file. It should generally stay the same unless the company has a weird format or you need to change out some specific words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2mAk37vqSM-z"
      },
      "outputs": [],
      "source": [
        "def captureGuidance(eightK_lst):\n",
        "    guidance_bs4_lst=[]\n",
        "    for ele in eightK_lst:\n",
        "\n",
        "        bs4_8K=extract_soup(ele)\n",
        "        #print(bs4_8K.prettify())\n",
        "        test= bs4_8K.get_text()\n",
        "        #print(len(test))\n",
        "        result = find_between( test,\"Financial Results\" , \"Recent Highlights\" )\n",
        "        print(result)\n",
        "        # if not result: # some companies call outlook guidance\n",
        "        #   result = find_between( test, \"Guidance\", \"Call\")\n",
        "        # elif not result: # pltr is inbetween outlook and earnings webcast\n",
        "        #   result = find_between( test, \"Outlook\", \"Earnings\")\n",
        "\n",
        "\n",
        "        #print(bs4_8K.select(test.name+':contains(\"Financial Outlook\")'))\n",
        "        #print(result)\n",
        "        #print(bs4_8K.find(text=\"Quarterly Conference Call \").previous_element.parent)\n",
        "\n",
        "        result = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', result)\n",
        "        result = re.sub(r'(\\$\\d+\\.\\d+)', r'\\1 ', result)\n",
        "        result = re.sub(r'\\b(low|mid|high|single|teens|to)-\\b', r'\\1 ', result)\n",
        "\n",
        "        replace = {'%':' % ','shares':' shares ','.For':' for','outstandingfull':' outstanding full ','20232exhibit':'2023 ','year over year':'year-over-year','millionfor':' million for ','\\n':' ',\n",
        "                '•': ' ','\\xa028':' ','\\xa017':' ','    ':'','◦':' ','million.':' million . ', '\\xa0':' ','%.': ' % .','\\uf0b7':' ','oRevenue':'revenue','oNon-GAAP':'non-gaap','●':' ','.Non-GAAP':'. non-gaap','.Revenues':'. revenues','breakeven':'$0'}\n",
        "        for word in replace:\n",
        "          result = result.replace(word,replace[word])\n",
        "        guidance_bs4_lst.append(result.strip().lower())\n",
        "    return guidance_bs4_lst\n",
        "\n",
        "metrics_data = captureGuidance(metrics_8k)\n",
        "metrics_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKO64TKAUZeW"
      },
      "source": [
        "These are the custom chunking model words. This will vary depending on the company, but the core words will stay in here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYwNsUJA6JlQ"
      },
      "source": [
        "Filter sentence function: Will need to change the stop words depending on the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t3C-uIY70-em"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = {'first': 'CD', 'second': 'CD', 'third': 'CD', 'fourth': 'CD', 'total': 'CD', 'expected': 'expected', 'quarter': 'TIME', 'year': 'TIME', 'breakeven': 'CD', 'full':\n",
        "'CD', 'tax': 'tax', 'subscription': 'sub', 'billings': 'bill', 'billion': 'unit', 'revenue': 'rev', 'unit': 'unit', 'full-year': 'TIME', 'fiscal': 'TIME', 'million': 'unit',\n",
        "'share': 'share', 'shares': 'share', 'year-over-year': 'yoy', '%': '%', 'ebitda': 'ebitda', 'loss': 'loss', 'arr': 'arr', 'income': 'income', 'net': 'net', 'revenues': 'rev',\n",
        "'non-gaap': 'non-gaap', 'gaap': 'gaap', 'operating': 'op', 'margin': 'mar', 'services': 'ser', 'adjusted': 'adj', 'negative': 'negative', 'respectively': 'resp', 'basic': 'basic',\n",
        "'q1': 'TIME', 'q2': 'TIME', 'q3': 'TIME', 'q4': 'TIME', 'fy': 'TIME', 'expenses': 'expenses', 'quarter-over-quarter': 'qoq', 'expectation ': 'expect', 'gross': 'gross', 'growth': 'growth',\n",
        "'rate': 'rate', 'operations': 'op', 'operation': 'op', 'cash': 'cash', 'expenditures': 'expenses', 'provision': 'prov', 'thousand': 'unit', 'earning': 'earn', 'taxes': 'tax', 'expense': 'expenses',\n",
        "'interest': 'interest', 'subscriptions': 'sub', 'eps': 'eps', 'compensation': 'comp', 'amortization': 'amort', 'acquisition': 'acq', 'costs': 'cost', 'debt': 'debt', 'assumed': 'ass', 'common': 'com',\n",
        "'dec.': 'month', 'break-even': 'CD', 'free': 'free', 'flow': 'flow', 'annual': 'ann', 'cloud': 'cloud', 'positive': 'pos', 'zero': 'CD', 'comparable': 'comp', 'comp': 'comp', 'return': 'rtn', 'invested': 'inv',\n",
        "'capital': 'cap', 'dividend': 'div', 'payments': 'pay', 'target': 'target', 'consolidated': 'con','percent:%'\n",
        "'repurchases': 'repur', 'repurchase': 'repur','repurchased': 'repur', 'repayments': 'repay', 'maturities': 'matur', 'reduced': 'reduce', 'reducing': 'reduce', 'planned': 'plan', 'percent': '%', 'investments': 'inv',\n",
        "'profit': 'profit', 'low-teens': 'CD', 'fromsource': 'src', 'organic': 'organic', 'expansion': 'growth', 'bps': 'bps', 'increase': 'growth', 'half': 'CD', 'back': 'CD', 'ending': 'end', 'ended': 'end', 'count': 'count',\n",
        "'even': 'CD', 'single': '%', 'percentage': '%', 'low': 'CD', 'mid': 'CD', 'high': 'CD', 'flat': 'CD', 'teens': '%', 'margins': 'mar', 'core': 'core', 'non-u.s.': 'non-gaap', 'outlook': 'outlook', 'â€”': ':', 'homes': 'home',\n",
        "'closed': 'closed', 'end': 'end', 'lower': 'reduce', 'sg': 'sg', 'pre-tax': 'pre-tax', 'increasing': 'growth', 'financial': 'financial', 'roic': 'roic', 'basis': 'basis', 'points': 'points', 'sales': 'sales', 'period': 'period',\n",
        " 'double-digit': 'CD', 'saas': 'saas', 'license': 'license', 'attributable': 'att', 'stockholders':  'stkhdr', 'hardware': 'hardware', 'reduce': 'reduce', 'stock': 'stk', 'restoration': 'restoration', 'continuing': 'continuing',\n",
        " 'intangibles': 'intan', 'non-cash': 'non-cash', 'stock-based': 'stk', 'service': 'ser', 'expectations': 'expect', 'aggregates': 'aggr', 'freight-adjusted': 'frtadj', 'price': 'price',\n",
        "'digit': '%', 'asphalt': 'asphalt', 'concrete': 'concrete', 'calcium': 'calcium', 'sag': 'sag', 'depreciation': 'dpr', 'depletion': 'dpl', 'accretion': 'accr', 'shipment': 'spmt', 'cost': 'cost',\n",
        "'down': 'negative', 'up': 'pos', 'volume': 'vol', 'tons': 'tons', 'spending': 'spend', 'pricing': 'price', 'non-aggregates': 'non-aggr', 'improvement': 'growth', 'improve': 'growth', 'diluted': 'dil', 'earnings': 'earn',\n",
        "'foreign':'foreign','grew':'growth' , 'grow':'growth' ,'finance':'fin','currency':'currency','expect':'expect','marketable':'marketable','securities':'securities','headcount':'headcount', 'ARR' : 'ARR'}\n",
        "\n",
        "model.update({'january': 'month', 'february': 'month', 'march': 'month', 'april': 'month', 'may': 'month', 'june': 'month','july': 'month', 'august': 'month', 'september': 'month', 'october': 'month', 'november': 'month', 'december': 'month'})\n",
        "#meta specific\n",
        "model.update({'ad':'ad','dap':'dap','daus':'daus','map':'map','maus':'maus','impressions':'impr'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zdLcu4Ft50id"
      },
      "outputs": [],
      "source": [
        "def filter_sent(sentence, model):\n",
        "    custom_stop = [\"approximately\",\"/\", ',','information','available',\"'s\",'follows',':','range','outstanding','respectively','average','basic','weighted-average','representing','weighted','ending','roughly','~','assuming', 'including', 'principal','payments','leases','projected']\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_sentence = []\n",
        "    for w in sentence:\n",
        "        if (w not in stop_words and w not in custom_stop) or (w in model.keys()):\n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SNzMoSe847Z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY2XKUeNU9Fa"
      },
      "source": [
        "Chunking function: This is the most difficult function to write. Completely depends on the way the company writes their sentences. When writing this function it's best to print(tagged) to see what's being shown. During this stage changes may need to be made to the model and custom stop words, as well as the captureGuidance function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qXMOa0ujVc66"
      },
      "outputs": [],
      "source": [
        "def metrics_CHUNK(sentence, model):\n",
        "\n",
        "    tagged_raw = nltk.pos_tag(sentence)\n",
        "\n",
        "    tagged = change_pos_tag(tagged_raw,model) #https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
        "    print(tagged)\n",
        "    chunk_rev = r\"\"\"\n",
        "    Timeline: {<TIME><CD><TIME><CD>}\n",
        "    Annual_ARR: <ann><VBG><rev><growth><CD><\\%><yoy><\\$>{<CD>}<unit>\n",
        "    gross_ret: <bill><growth>{<CD>}<%>\n",
        "    Next_Gen_SIEM:<arr><growth><CD><%><yoy><\\$>{<CD>}<unit>\n",
        "    total_subs_customers: <JJ><NN><arr><growth>{<CD>}<%>\n",
        "\"\"\"\n",
        "    chunkParserrev = nltk.RegexpParser(chunk_rev)\n",
        "    chunkedrev = chunkParserrev.parse(tagged)\n",
        "\n",
        "\n",
        "    return listofdict(chunkedrev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JdvD40LVpuJ"
      },
      "source": [
        "Outlook function: I don't think this ever changes but I have only done one text company so far so I am including it here: This should end up giving a list of dictionaries that grouped together correctly. Make sure to check that the output is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wLqjpDifXIDh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def metrics_info(data):\n",
        "    Total_Outlook = []\n",
        "    for i in range(len(data)):\n",
        "        if data[i] != '':\n",
        "            list_dict_chunks = metrics_CHUNK(filter_sent(word_tokenize((data[i])),model),model)\n",
        "            Total_Outlook.extend(combine_dict(list_dict_chunks))\n",
        "\n",
        "    return Total_Outlook\n",
        "\n",
        "raw_info = metrics_info(metrics_data)\n",
        "raw_info\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMWaigutXgJl"
      },
      "source": [
        "Process function: Once all the data is in the correct dictionary, it needs to go through this process function to extract specifically the numbers. Uses the unitConversion function that's defined higher up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R9-iYKOCYKXG"
      },
      "outputs": [],
      "source": [
        "def process(raw_info_dict): #If you ever find an error with this function let @Maxime know ASAP\n",
        "    temp_dict = {}\n",
        "    if \"Drop\" in raw_info_dict.keys():\n",
        "      raw_info_dict.pop(\"Drop\")\n",
        "\n",
        "    if len(raw_info_dict)>1:\n",
        "        temp_dict = {}\n",
        "        temp_dict[\"Timeline\"] = raw_info_dict[\"Timeline\"]\n",
        "\n",
        "        for key, value in raw_info_dict.items():\n",
        "          value = value.replace(\"low teens\", \"11 % 13 %\")\n",
        "          value = value.replace(\"high teens\",\"17 % 19 %\" )\n",
        "          value = value.replace(\"mid teens\", \"14 % 16 %\")\n",
        "          value = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', value)#remove dash between two numbers\n",
        "          arr = value.split()\n",
        "          prefix = 1\n",
        "          vals = []\n",
        "          if (len(arr) == 0 or key==\"Timeline\"):\n",
        "            continue\n",
        "          conversions = [] #case where there are multiple conversions in the same captured data\n",
        "          conversion = ''\n",
        "          #Find unit conversion\n",
        "          for i, elem in enumerate(arr):\n",
        "            if elem == 'breakeven' or elem == 'break-even' or elem == 'zero': #replace words meaning 0\n",
        "              arr[i] = '0'\n",
        "            if elem in unit_conv.keys():\n",
        "              conversion = elem\n",
        "              conversions.append(conversion)\n",
        "          # Check if unit conversion is in key\n",
        "          if conversion == '':\n",
        "            for word in key.split():\n",
        "              if word.replace(\"(\",\"\").replace(\")\",\"\") in unit_conv.keys(): #Replace unit conversion in key, ie. if key is \"Revenue (in millions)\", it should be \"Revenue\"\n",
        "                conversion = word.replace(\"(\",\"\").replace(\")\",\"\")\n",
        "                conversions.append(conversion)\n",
        "\n",
        "          if len(conversions) == 0:\n",
        "            conversions.append('')\n",
        "\n",
        "          if len(set(conversions)) > 1:\n",
        "            multi = True\n",
        "          else:\n",
        "            multi = False\n",
        "\n",
        "          # regex finds () and [] brackets\n",
        "          key = re.sub(\"\\(.*?\\)|\\[.*?\\]\",\"\", key)\n",
        "          #Check if numbers should be negative\n",
        "          if (\"loss\" in arr) or (\"negative\" in arr) or (\"decrease\" in arr):\n",
        "            prefix = -1\n",
        "          conv_num = 0\n",
        "          #Find values (should be 1 or 2)\n",
        "\n",
        "\n",
        "          # IMPROVED HANDLING OF NEGATIVE AND POSITIVE VALS IN SAME CHUNK - by Jack Li\n",
        "          ignore_set = ['$']\n",
        "          switch = False\n",
        "          negative = False\n",
        "          state = 0\n",
        "          for elem in arr:\n",
        "            if \"(\" in elem and state == 0:\n",
        "              state = 1\n",
        "            if (elem.replace(\".\",\"\").replace(\",\",\"\").replace(\"$\",\"\").replace('(','').replace(')','').replace('%','').isdigit() and state == 1) or (elem == 'loss' and state == 1):\n",
        "              state = 2\n",
        "            if \")\" in elem and state == 1:\n",
        "              state = 0\n",
        "            if \")\" in elem and state == 2:\n",
        "              switch = True\n",
        "              break\n",
        "\n",
        "          pos_search = False\n",
        "\n",
        "          if switch is False:\n",
        "            for elem in arr:\n",
        "              if elem == 'positive':\n",
        "                pos_search = True\n",
        "                continue\n",
        "              if pos_search:\n",
        "                if (not elem.replace(\".\",\"\").replace(\",\",\"\").replace(\"$\",\"\").replace('(','').replace(')','').replace('%','').replace('-','').replace('~','').isdigit()) and (elem not in ignore_set):\n",
        "                  pos_search = False\n",
        "\n",
        "              if elem.replace(\".\",\"\").replace(\",\",\"\").replace(\"$\",\"\").replace('(','').replace(')','').replace('%','').replace('-','').replace('~','').isdigit():\n",
        "                if pos_search:\n",
        "                  vals.append(unitConversion([elem, conversions[conv_num]]))\n",
        "                  if multi: conv_num += 1\n",
        "                  pos_search = False\n",
        "                else:\n",
        "                  vals.append(unitConversion([elem, conversions[conv_num]])*prefix)\n",
        "                  if multi: conv_num += 1\n",
        "          else:\n",
        "            for elem in arr:\n",
        "              if \"(\" in elem:\n",
        "                negative = True\n",
        "              if \")\" in elem:\n",
        "                negative = False\n",
        "              if elem.replace(\".\",\"\").replace(\",\",\"\").replace(\"$\",\"\").replace('(','').replace(')','').replace('%','').replace('-','').replace('~','').isdigit():\n",
        "                if (negative):\n",
        "                  vals.append(unitConversion([elem, conversions[conv_num]])*-1)\n",
        "                  if multi: conv_num += 1\n",
        "                else:\n",
        "                  vals.append(unitConversion([elem, conversions[conv_num]]))\n",
        "                  if multi: conv_num += 1\n",
        "\n",
        "          if max(vals)-min(vals) > 100 * unit_conv[conversion]:\n",
        "            for val in vals:\n",
        "              if val/unit_conv[conversion] in years:\n",
        "                  vals.remove(val)\n",
        "                  break\n",
        "\n",
        "          temp_dict[key] = max(vals)\n",
        "          #temp_dict[key+\" Lower\"] = min(vals)\n",
        "\n",
        "    return temp_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcaAD-3fZlbM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I7Cla6fCZhJ3"
      },
      "outputs": [],
      "source": [
        "def clean(raw_info):\n",
        "    Outlook = []\n",
        "    for i in range(len(raw_info)):\n",
        "        if process(raw_info[i]) != None:\n",
        "            Outlook.append(process(raw_info[i]))\n",
        "    return Outlook\n",
        "\n",
        "data = clean(raw_info)\n",
        "\n",
        "cleaned_data = [i for i in data if len(i)>1]\n",
        "cleaned_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RuYcKJgyyDrm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tvuZOSB-0zXT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}